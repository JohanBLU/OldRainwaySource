version=pmwiki-2.2.85 ordered=1 urlencoded=1
author=mfwolff
charset=UTF-8
csum=added a link
ctime=1380201392
name=PmWikiDe.Robots
rev=3
targets=PmWikiDe.AvailableActions,PmWikiDe.WikiStyles,PITS.00563,PmWikiDe.LayoutVariables,Cookbook.ControllingWebRobots,PmWiki.Robots,Category.Robots
text=(:title Web-Roboter:)%0a(:Summary: Umgang mit Web-Robotern, Web-Spidern und Suchmaschinen :)%0a(:Original_Page: PmWiki.{$Name}:)%0a(:Audience: Autoren, Administratoren:)%0a(:Translation_Date: 2013-09-26:)%0a(:Translation_Status: %25green%25fertig%25%25:)%0a%0a!!Nofollow links%0a%0aDas Standard-PmWiki-Skin fügt zu allen [[AvailableActions|verfügbaren Aktionen]] ($[View], $[Edit], $[History], $[Attach], $[Print] und $[Backllinks])  "rel=nofollow" hinzu. Das ist eine Vereinbarung, die von Google eingeführt wurde und die Web-Robotern anweist, den Verweisen nicht zu folgen und den Inhalten, auf die mit dem Verweis zugegriffen wird, keine Bedeutung zuzumessen. '^[[http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html|#]]^'%0a%0aBeachten Sie, dass viele Roboter, einschließlich Yahoo! Slurp und msnbot, das "nofollow"-Attribut in den Verweisen vollständig missachtet. %0a%0aDie Anti-Spam-Vereinbarung, die "nofollow" zuerst eingeführt hat, sagte nichts aus darüber, dass Roboter dem Verweis nicht folgen, sondern nur, dass Verweise mit "rel=nofollow" in Suchergebnissen keine Gewichtung erhalten sollen.%0a%0aSie können selbst rel=nofollow zu jedem ihrer Verweise hinzufügen, indem Sie die [[WikiStyles|+]] verwenden:%0a->[@%25rel=nofollow$[[Ihr Verweis]]%25%25@]%0a%0a!!Die Datei @@robots.txt@@%0a%0aHier ist ein Beispiel für den Inhalt einer ''robots.txt''-Datei, das vor Allem den Robotern sagt, sie mögen Verweisen mit 'action=' nicht folgen:%0a%0a-> [@%0aUser-agent: *%0aDisallow: /pmwiki.php/Main/AllRecentChanges%0aDisallow: /pmwiki.php/%0aDisallow: */search%0aDisallow: SearchWiki%0aDisallow: *RecentChanges%0aDisallow: RecentChanges%0aDisallow: *action=%0aDisallow: action=%0aUser-Agent: W3C-checklink%0aDisallow:%0a@]%0a%0a!!Roboter Variablen%0aWir können nun bessere Kontrolle über Roboter(webcrawler)-Interaktionen mit einer Site unterstützen, um die Serverlast zu verringern.%0a:$RobotPattern: wird genutzt, um Roboter auf Basis des ''user-agent strings'' zu identifizieren.%0a:$RobotActions: Jede Aktion, die nicht in dieser Liste enthalten ist, führt zu einer ''403 Forbidden response to robots''-Meldung%0a:$EnableRobotCloakActions: Setzen dieses Flags eliminiert jede verbotene Aktion (?action=values) von den Seitenverweisen, die an die Roboter gemeldet werden, was die Bandbreitenbelastung durch Roboter noch weiter reduzieren wird  [-(PITS:00563)-].%0a:$MetaRobots: see [[layout variables|+]]%0a%0a!!Siehe auch %0a* [[Cookbook:Controlling WebRobots]] - {Cookbook/ControllingWebRobots$:Summary}%0a* %25newwin rel=nofollow%25[[http://robotstxt.org/]]%0a* %25newwin rel=nofollow%25[[http://sitemaps.org/]]%0a* %25newwin rel=nofollow%25[[https://support.google.com/webmasters/answer/96569?hl=de]]%0a* %25newwin rel=nofollow%25Wikipedia:Robots.txt Robots Exclusion Standard%0a* %25newwin rel=nofollow%25http://microformats.org/wiki/rel-nofollow, http://www.nonofollow.net/%0a* Category: [[!Robots]]%0a
time=1459804955
title=Web-Roboter
